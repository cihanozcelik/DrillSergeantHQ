# Pipeline Overview: Rollout → GAE → PPO → Publish

Training becomes understandable once you can name its stages.

DrillSergeantHQ’s training loop is a pipeline that repeats forever:

1. **Rollout**: generate experience from many headless envs
2. **Compute targets**: advantages (GAE) + returns
3. **Optimize**: PPO updates over minibatches/epochs
4. **Publish**: write new weights to the SAB double buffer
5. **Observe**: export telemetry so humans can see health and progress

## The pipeline in one page

```text
Rollout workers (CPU)              Trainer worker (GPU/CPU)
---------------------              ------------------------
step env batches                   read rollout chunks
infer actions                      compute GAE + returns
write experience to SAB ring  ---> shuffle into minibatches
                                   PPO update (E epochs)
                                   publish checkpoint (SAB)
                                   send metrics (1–2 Hz)
```

## Stage 1: Rollout generation (the firehose)

Rollouts are generated by headless env batches because:

- on-policy algorithms need fresh experience
- throughput dominates learning speed

Key outputs per step:

- observation \(s_t\)
- action \(a_t\)
- reward \(r_t\)
- done flag
- log-prob \(\log \pi(a_t|s_t)\)
- baseline value \(V(s_t)\)

These last two are crucial: PPO uses them to compute stable updates.

## Stage 2: GAE + returns (making training targets)

GAE computes a smoothed advantage signal \(A_t\) from rewards and values.

Returns \(R_t\) (for the value function) are typically:

\[
R_t = A_t + V(s_t)
\]

## Stage 3: PPO update (the controlled step)

PPO is designed to avoid “policy lurches.” It constrains the update so the new policy doesn’t move too far from the old one—without needing second-order optimization.

In practice:

- shuffle rollouts into minibatches
- run a few epochs (2–4 is common)
- compute clipped surrogate loss + value loss + entropy bonus
- apply Adam updates

## Stage 4: Publish (turn learning into visible behavior)

Publishing is a system contract, not an ML detail:

- weights are written into the inactive buffer
- atomics flip active index + bump version
- show worker hot-swaps on version change

This stage is what turns “training” into the product experience.

## Stage 5: Observe (if you can’t see it, you can’t trust it)

Training without telemetry is a black box.

We export:

- losses (policy/value/entropy)
- performance (steps/sec, updates/sec)
- environment metrics (avg reward, win-rate EMA, episode length)
- hyperparameters (lr, clip epsilon)

Telemetry is not garnish. It is a core subsystem.

---

**Prev:** [Part V — Training System](README.md)  
**Next:** [PPO Objective (Clipped Surrogate)](ppo-objective.md)


