# PPO Objective (Clipped Surrogate)

PPO is used here for a pragmatic reason: it is stable enough to run continuously while the user watches.

You can think of PPO as a system-friendly compromise:

- it gives you the throughput of first-order optimization
- it avoids catastrophic policy updates by constraining change

## The ratio that measures “how much did the policy change?”

Given rollouts generated by the old policy \(\pi_{old}\), define:

\[
ratio_t = \exp(\log \pi_{new}(a_t|s_t) - \log \pi_{old}(a_t|s_t))
\]

If \(ratio_t = 1\), nothing changed for that sampled action. If it drifts too far, the update becomes risky.

## The clipped surrogate objective

Let \(A_t\) be the advantage estimate.

\[
L_{clip} = -\mathbb{E}\left[\min(ratio_t \cdot A_t,\ \mathrm{clip}(ratio_t, 1-\epsilon, 1+\epsilon)\cdot A_t)\right]
\]

Interpretation:

- if \(ratio_t\) tries to move outside \([1-\epsilon, 1+\epsilon]\), we clamp it
- this prevents the optimizer from exploiting a few samples to make a huge step

## The full PPO loss used in practice

We combine three terms:

- policy loss \(L_{clip}\)
- value loss \(L_v\)
- entropy bonus \(L_{ent}\) (encourages exploration)

\[
L = L_{clip} + c_v \cdot \mathbb{E}[(V(s_t)-R_t)^2] - c_e \cdot \mathbb{E}[H(\pi(\cdot|s_t))]
\]

Notes:

- signs differ by convention (maximize vs minimize). The important thing is consistent implementation.
- entropy is usually *subtracted* from the minimized loss (because we want *more* entropy early on).

## Why PPO is “engine-friendly”

From a systems perspective, PPO is valuable because:

- the update is bounded (clipping)
- telemetry is meaningful (loss terms correlate with collapse modes)
- you can do small, frequent updates (good for “live improvement”)

## Failure modes (what to watch)

- **Policy collapse**: entropy drops to ~0, behavior becomes brittle  
  - response: increase entropy coefficient, adjust learning rate, improve reward shaping

- **Value divergence**: value loss explodes or becomes NaN  
  - response: check normalization, check returns/GAE, validate kernels against CPU reference

- **Too-conservative learning**: ratio stays near 1 and reward doesn’t improve  
  - response: increase learning rate slightly, adjust clip epsilon, improve rollout diversity

PPO is not magic. It is a controlled update mechanism that plays well with continuous publishing and human-facing UX.

---

**Prev:** [Pipeline Overview: Rollout → GAE → PPO → Publish](pipeline-overview.md)  
**Next:** [GAE and Returns](gae.md)


